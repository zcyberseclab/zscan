# vLLM 漏洞信息

## 概述
vLLM 是一个高性能的 LLM 推理和服务库，用于部署和运行大型语言模型。

## 常见安全问题

### 1. 未授权访问
- **描述**: vLLM 服务可能未配置身份验证，导致未授权访问
- **影响**: 可以查看模型列表、调用模型进行推理
- **检测**: 访问 `/v1/models` 或 `/health` 端点

### 2. 信息泄露
- **描述**: API 文档或模型信息可能泄露敏感信息
- **影响**: 泄露模型配置、系统信息等
- **检测**: 访问 `/docs` 或 `/v1/models` 端点

### 3. SSRF 漏洞
- **描述**: 模型加载功能可能存在 SSRF 漏洞
- **影响**: 可能导致内网探测、信息泄露
- **检测**: 尝试通过模型名称参数触发 SSRF

## 默认端口
- 8000 (vLLM)
- 8080 (vLLM/TGI)

## 相关 CVE
暂无公开 CVE，但存在未授权访问等安全问题。

